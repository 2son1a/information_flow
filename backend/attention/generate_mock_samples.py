"""
This script generates mock sample attention data for each model.
It creates the same structure that would be generated by the real models,
but with artificial data that doesn't require loading the actual models.
"""
import json
import os
import random
import numpy as np

# Match the available models from model.py
AVAILABLE_MODELS = {
    "gpt2-small": {"layers": 12, "heads": 12},
    "pythia-2.8b": {"layers": 32, "heads": 32}
}

def generate_mock_attention_patterns(model_name, num_tokens=10):
    """Generate mock attention patterns for a model."""
    model_config = AVAILABLE_MODELS[model_name]
    num_layers = model_config["layers"]
    num_heads = model_config["heads"]
    
    # Generate tokens
    tokens = [f"token_{i}" for i in range(num_tokens)]
    
    # Generate random attention patterns
    attention_patterns = []
    
    for layer in range(num_layers):
        for head in range(num_heads):
            head_type = random.choice(["induction", "previous_token", "name_mover", "duplicate_token", "unknown"])
            
            # For each token pair, create attention pattern
            for src_idx in range(num_tokens):
                for dest_idx in range(num_tokens):
                    # Only allow attending to previous tokens (causal mask)
                    if dest_idx >= src_idx:
                        # Generate a random weight with higher values for some head types
                        if head_type == "name_mover" and random.random() > 0.7:
                            weight = random.uniform(0.6, 0.9)
                        else:
                            weight = random.uniform(0, 0.5)
                            
                        attention_patterns.append({
                            "sourceLayer": layer,
                            "sourceToken": src_idx,
                            "destLayer": layer + 1,
                            "destToken": dest_idx,
                            "weight": weight,
                            "head": head,
                            "headType": head_type
                        })
    
    return {
        "numLayers": num_layers + 1,  # +1 for output layer
        "numTokens": num_tokens,
        "numHeads": num_heads,
        "tokens": tokens,
        "attentionPatterns": attention_patterns,
        "model_name": model_name,
        "model_info": {
            "name": model_name,
            "layers": num_layers,
            "heads": num_heads,
            "architecture": model_name
        }
    }

def generate_mock_sample_data():
    """Generate mock sample data for each available model."""
    # Sample text (just for reference)
    text = "When Mary and John went to the store, John gave a drink to"
    
    # Create output directory if it doesn't exist
    output_dir = "../../app/data"
    os.makedirs(output_dir, exist_ok=True)
    
    # Generate sample data for each available model
    for model_name, config in AVAILABLE_MODELS.items():
        print(f"\nGenerating mock sample data for model: {model_name}")
        
        # Generate mock attention patterns
        result = generate_mock_attention_patterns(model_name)
        
        # Save to model-specific JSON file
        output_path = f"{output_dir}/sample-attention-{model_name}.json"
        with open(output_path, "w") as f:
            json.dump(result, f, indent=2)
        
        print(f"Mock sample attention patterns saved to {output_path}")
        print(f"Number of tokens: {result['numTokens']}")
        print(f"Number of attention patterns: {len(result['attentionPatterns'])}")

if __name__ == "__main__":
    generate_mock_sample_data() 